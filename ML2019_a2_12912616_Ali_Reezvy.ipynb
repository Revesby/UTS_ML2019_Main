{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31005 Machine Learning Spring 2019 \n",
    "## Assignment 2: Practical Machine Learning Project\n",
    "### ID3 Algorithm Implementation\n",
    "#### Reezvy Ali 12912616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Github Plain Text URL__ : https://github.com/Revesby/UTS_ML2019_Main/blob/master/ML2019_a2_12912616_Ali_Reezvy.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The aim of this report is to implement an Iterative Dichotomiser 3 (ID3) algorithm from scratch to read training data and test data respectively in order to predict a particular outcome. The implementation of the ID3 algorithm in this report will centre on the renowned 'Mushroom' dataset which is derived from ‘The Audubon Society Field Guide to North American\n",
    "Mushrooms (G. H. Lincoff, 1981). We will train an algorithm and implement ID3 on a test dataset to classify whether a mushroom is edible or poisonous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://datascienceplus.com/wp-content/uploads/2018/02/mushroom-glossary.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://datascienceplus.com/wp-content/uploads/2018/02/mushroom-glossary.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set will include 23 attributes that describe different features of mushrooms. These include odour, spore print colour and if the mushroom is poisonous or edible. The test set will include all of the features excluding the target class (poisonous or edible) which is the outcome we are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "The ID3 algorithm operates as a decision tree which splits on nodes by evaluating information gain and entropy. The ID3 algorithm was invented in 1986 by Ross Quinlan and is useful as its design can handle discrete values, which is relevant to the mushroom study this report will focus on.\n",
    "\n",
    "**Entropy** is a measure of uncertainty within the dataset and information gain is how much uncertainty is reduced in some set X after splitting on the node Y. **Information gain** is calculating using entropy and the formulas are provided as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://computersciencesource.files.wordpress.com/2010/01/entropycalc1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://computersciencesource.files.wordpress.com/2010/01/entropycalc1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where\n",
    "+ H – Entropy\n",
    "+ p(i) – The proportion of the number of elements in class i to the number of elements in a given set.\n",
    "\n",
    "\n",
    "The set for entropy calculation changes during every iteration of the ID3 algorithm. When _H = 0_ (entropy), the set is perfectly classified. Given the nature of our study, the entropy will only range between 0-1 as edible/poisonous is inherently binary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQzflnv6WaDVhi22YfZK2c8nEMY1wIS9TH4fA5Vu1LKcJslYCdj\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQzflnv6WaDVhi22YfZK2c8nEMY1wIS9TH4fA5Vu1LKcJslYCdj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S represents the subset before any splitting, and D represents the possible outcomes of splitting using a given attribute and condition. V assumes that all the values D can be measured one by one. The parallel lines on V and S denote the size of the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Determining the split**\n",
    "Entropy is calculated on all attributes. The minimum entropy will determine where the split will occur on some given set.\n",
    "\n",
    "Alternatively, if information gain is used as a reference for splitting, the node with the highest information gain will determine the split in that iteration for a given set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "## ID3 Algorithm Data Model Design\n",
    "\n",
    "The ID3 algorithm stores the decision tree as a **nested dictionary**, which operates in similar fashion to a standard dictionary (key-value structure) with the difference being that key-value pairs can be nested inside as well.\n",
    "\n",
    "The outer-most dictionary in the nested dictionary structure represents the _root split_ of the tree and therefore only has one key-value pair as it is the intial starting point of the decision tree. The actual nesting begins in the value part of the root's key-value pair.\n",
    "\n",
    "The key-value pair in the ID3 algorithm represents the attribute and it's classification. Once entropy can no longer be reduced (i.e. uncertainty can no longer be decreased for the set), splits will stop occuring and hence nesting of the dictionary will stop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "We import libraries that will implement the ID3 algorithm. These will help us with:\n",
    "+ Data wrangling\n",
    "+ Math modules\n",
    "+ Visualising the actual ID3 algorithm and it's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "import pydot\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data, identifying attributes and class column\n",
    "\n",
    "A function, __import_data__, will allow the ID3 algorithm to separate the target class column from the other attributes in the mushroom data whilst the data is being imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "             data = list(csv.reader(f)) \n",
    "                                \n",
    "    attributes = data.pop(0)         \n",
    "    attributes.remove(\"class\") \n",
    "    target_column = [i.pop(0) for i in data] '\n",
    " \n",
    "    set_1 = len(data[0]) == len(attributes) \n",
    "    set_2 = len(data) == len(target_column) \n",
    "    \n",
    "    if set_1 and set_2:\n",
    "        return data, target_column, attributes \n",
    "    else:\n",
    "        print(\"Function import_data not working, ERROR\") \n",
    "        \n",
    "        exit(-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding entropy and information gain formulas\n",
    "\n",
    "##### Entropy\n",
    "\n",
    "Entropy for the target class (edible/poisonous) can be made with two functions: __entropy_formula__ and __entropy_target__.\n",
    "Entropy_formula contains the formula for entropy mentioned earlier. Entropy_target calculates the entropy of the edibility target class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_formula(p): ## Iterates through all percentages in between 0 and 1\n",
    "     if p != 0:\n",
    "        return -p * np.log2(p)\n",
    "     else:\n",
    "        return 0\n",
    "    \n",
    "def entropy_target(target_column): # The entropy of the outcome of whether the mushroom is edible or not is calculated here\n",
    "    unique_outcome = {} \n",
    "    n = len(target_column) \n",
    "    for i in target_column: \n",
    "        if i not in unique_outcome: \n",
    "            unique_outcome[i] = 1\n",
    "        else:\n",
    "            unique_outcome[i] = unique_outcome[i] + 1 \n",
    "    entropy = 0\n",
    "    for i in unique_outcome:\n",
    "\n",
    "        p = unique_outcome[i] / float(n) \n",
    "        entropy = entropy + entropy_formula(p) \n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Information Gain\n",
    "Information gain will be implemented using the formula mentioned earlier, as well as using the entropy_formula function to calculate the information gain of each attribute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(data, outcomes, attribute):\n",
    "    gain = 0\n",
    "    n_d = len(data)\n",
    "\n",
    "    gx = {} \n",
    "    for i in data:\n",
    "        if i[attribute] not in gx.keys():\n",
    "            gx[i[attribute]] = 1\n",
    "        else:\n",
    "            gx[i[attribute]] = gx[i[attribute]] + 1\n",
    "    for f in gx.keys():\n",
    "        attribute_entropy = 0\n",
    "        row_index = 0\n",
    "        new_outcome = {}\n",
    "        count_outcome = 0\n",
    "\n",
    "        for i in data:\n",
    "            if i[attribute] == f:\n",
    "                count_outcome = count_outcome + 1\n",
    "                if outcomes[row_index] in new_outcome.keys():\n",
    "                    new_outcome[outcome[row_index]] = new_outcome[outcomes[row_index]] + 1\n",
    "                else:\n",
    "                    new_outcome[outcomes[row_index]] = 1\n",
    "            row_index = row_index + 1\n",
    "\n",
    "      \n",
    "        for c in new_outcome.keys():\n",
    "            attribute_entropy = attribute_entropy + entropy_formula(float(new_outcome[c]) / count_outcome)\n",
    "            \n",
    "        gain = gain + float(gx[f]) / n_d * attribute_entropy\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning, creating the sets and the decision tree\n",
    "##### Subset\n",
    "\n",
    "To support the iteration of the ID3 algorithm, subsets need to be created in order to partition the dataset. The subsets get smaller after each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_partition(data, target_column, attribute, f):\n",
    "    new_d = []\n",
    "    new_target = []\n",
    "    n_attributes = len(data[0])\n",
    "    row_index = 0\n",
    "   \n",
    "    for i in data:\n",
    "        if i[attribute] == f:\n",
    "            if attribute == 0:\n",
    "                new_row = i[1:]\n",
    "            elif attribute == n_attributes:\n",
    "                new_row = i[:-1]\n",
    "            else:\n",
    "                new_row = i[:attribute]\n",
    "                new_row.extend(i[attribute + 1:])\n",
    "\n",
    "            new_d.append(new_row)\n",
    "            new_target.append(target_column[row_index])\n",
    "        row_index = row_index + 1\n",
    "    return new_target, new_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree\n",
    "\n",
    "The decision tree can be made with a function __create_tree__ . The previous functions will be used in the development of the decision tree to determine splits, calculate and evaluate the biggest information gains as well as decreases in entropy (uncertainty).\n",
    "\n",
    "Before visualisation of the tree, the tree can be printed into text with another function __print_tree__ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree(data, outcomes, attributes):\n",
    "\n",
    "    n_d = len(data)\n",
    "    n_attributes = len(attributes)\n",
    "    vunique = {} \n",
    "    for value in outcomes: \n",
    "        if value in vunique.keys():\n",
    "            vunique[value] = vunique[value] + 1\n",
    "        else:\n",
    "            vunique[value] = 1\n",
    "\n",
    "    deflt = max(vunique, key=vunique.get)\n",
    "    if n_d == 0 or n_attributes == 0:\n",
    "        return deflt\n",
    "    elif len(np.unique(outcomes)) == 1:\n",
    "        return outcomes[0]\n",
    "    else:\n",
    "        tot_entropy = entropy_target(outcomes) \n",
    "        inform_gain = np.zeros(n_attributes)\n",
    "        for attribute in range(n_attributes):\n",
    "            inf = information_gain(data, outcomes, attribute)\n",
    "            inform_gain[attribute] = tot_entropy - inf\n",
    "        best_attribute = np.argmax(inform_gain) \n",
    "        gxs = np.unique(np.transpose(data)[best_attribute])\n",
    "        g = attributes.pop(best_attribute) \n",
    "        tree = {g: {}}\n",
    "        for gx in gxs:\n",
    "            target, data = subset(data, outcomes, best_attribute, gx)\n",
    "            sub_tree = create_tree(data, target, attributes) \n",
    "            tree[g][gx] = sub_tree\n",
    "        return tree\n",
    "    \n",
    "    \n",
    "def print_tree(tree, node):\n",
    "\n",
    "\n",
    "\n",
    "    tree_dict = {}\n",
    "    for root in tree.keys():\n",
    "        split = []\n",
    "        leaves = {}\n",
    "        split_value = []\n",
    "        for k in tree[root]:\n",
    "            if type(tree[root][k]) is not dict:\n",
    "                leaf = tree[root][k]\n",
    "                if leaf in leaves:\n",
    "                    leaves[leaf].append(k)\n",
    "                else:\n",
    "                    leaves[leaf] = [k] =\n",
    "            else: =\n",
    "                split_value.append(k)\n",
    "                print_tree(tree[root][k], k)\n",
    "\n",
    "    if len(split_value) != 0: # PRINTS SPLIT NODE\n",
    "        split.extend(split_value) #\n",
    "    split.append(leaves) # Append value where split occurs\n",
    "    tree_dict[root] = split\n",
    "    print(node, tree_dict, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to return the values of the target class, a function __tree_values_output__ will be made. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_values_output(tree, row, attributes):  \n",
    "    if type(tree) is not dict: \n",
    "        return str(tree)\n",
    "    else:\n",
    "        for k in tree.keys():\n",
    "            attribute_index = attributes.index(k)\n",
    "            attribute_i = row[attribute_index]\n",
    "            return tree_values_output(tree[k][attribute_i], row, attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training & Testing\n",
    "The following functions will be used in order to train the ID3 algorithm in a new function called __training__:\n",
    "+ __import_data__\n",
    "+ __create_tree__\n",
    "\n",
    "Although accuracy is not the best effectiveness metric of a learning model, it will be implemented using another function __authentication__ that will compare the mushroom training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(csv_file):\n",
    "    data, target_column, attributes = import_data(filename=csv_file)\n",
    "    tree = create_tree(data, target_column, attributes) \n",
    "    return tree \n",
    "\n",
    "\n",
    "def authentication(filename, tree):\n",
    "    with open(filename, 'r') as f: \n",
    "        data = list(csv.reader(f)) \n",
    "    attributes = data.pop(0) \n",
    "    attributes.remove(\"class\") \n",
    "    target_column = [i.pop(0) for i in data] \n",
    "\n",
    "    x = 0\n",
    "    n = len(data)\n",
    "    row_num = 0\n",
    "    output_vector = []\n",
    "\n",
    "    for i in data:\n",
    "        out = tree_output(tree, i, attributes) \n",
    "        output_vector.append(out) \n",
    "        if out == target_column[row_num]: \n",
    "            x = x + 1\n",
    "        row_num = row_num + 1\n",
    "    return x/float(n) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation\n",
    "\n",
    "After having defined the numerous functions needed, a final function will be created to import the mushroom training and testing datasets calculate relevant metrics and produce a decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final():\n",
    "    mush_train = \"mush_train.csv\" \n",
    "    mush_test = \"mush_test.csv\" \n",
    "    decision_tree = training(mush_train)\n",
    "    accuracy = authenticate(mush_test, dec_tree) \n",
    "    print(\"Accuracy:\", accuracy, \"\\n\")\n",
    "    print(\"DECISION TREE:)\")\n",
    "    print_tree(dec_tree) \n",
    "\n",
    "final()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The execution of the code originally ran smoothly however some bugs encountered and attempts at resolving these bugs were unsuccessful. Hence, a previous capture of the results are shown below which can be replicated with comprehensive bug fixes of the code and data.\n",
    "\n",
    "Accuracy is yielded as 100%. This indicates that there is overfitting in the results and may point to other issues. As mentioned earlier in the report, the results use nested dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/54nrJ36.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://i.imgur.com/54nrJ36.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In conclusion, the ID3 model was originally implemented successfully on the mushroom data however some issues were present. On reflection, the training and testing data should've been completely separate datasets instead of using the training dataset as a subset of the testing data. Ensuring these were distinct would have yielded a more meaningful accuracy to evaluate the effectiveness of the model.\n",
    "\n",
    "The code and bugfixing should've been given more time and effort so that the code could properly execute the results instead of using prior screenshots however external time constraints prevented this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethical\n",
    "Social and ethical applications of the ID3 algorithm can be used in many fields. For example, the mushroom study in this report is a valuable model as it attempts to identify dangerous food items amongst safe ones in a simple scenario. Implementing this type of model on real-life unknown mushrooms in the future may prove beneficial in predicting the outcome of contact before running a hard trial-and-error.\n",
    "\n",
    "In similar fashion, the ID3 algorithm can be implemented on other common discrete valued outcome scenarios and may provide further insights across different industries and fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Pitch\n",
    "[Available in the following Google Drive URL](http://tiny.cc/rali31005a2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
