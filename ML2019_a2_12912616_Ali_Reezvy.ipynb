{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31005 Machine Learning Spring 2019 \n",
    "## Assignment 2: Practical Machine Learning Project\n",
    "### ID3 Algorithm Implementation\n",
    "#### Reezvy Ali 12912616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The aim of this report is to implement an Iterative Dichotomiser 3 (ID3) algorithm from scratch to read training data and test data respectively in order to predict a particular outcome. The implementation of the ID3 algorithm in this report will centre on the renowned 'Mushroom' dataset which is derived from ‘The Audubon Society Field Guide to North American\n",
    "Mushrooms (G. H. Lincoff, 1981). We will train an algorithm and implement ID3 on a test dataset to classify whether a mushroom is edible or poisonous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://datascienceplus.com/wp-content/uploads/2018/02/mushroom-glossary.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://datascienceplus.com/wp-content/uploads/2018/02/mushroom-glossary.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set will include 23 attributes that describe different features of mushrooms. These include odour, spore print colour and if the mushroom is poisonous or edible. The test set will include all of the features excluding the target class (poisonous or edible) which is the outcome we are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "The ID3 algorithm operates as a decision tree which splits on nodes by evaluating information gain and entropy. The ID3 algorithm was invented in 1986 by Ross Quinlan and is useful as its design can handle discrete values, which is relevant to the mushroom study this report will focus on.\n",
    "\n",
    "**Entropy** is a measure of uncertainty within the dataset and information gain is how much uncertainty is reduced in some set X after splitting on the node Y. **Information gain** is calculating using entropy and the formulas are provided as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://computersciencesource.files.wordpress.com/2010/01/entropycalc1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://computersciencesource.files.wordpress.com/2010/01/entropycalc1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where\n",
    "+ H – Entropy\n",
    "+ p(i) – The proportion of the number of elements in class i to the number of elements in a given set.\n",
    "\n",
    "\n",
    "The set for entropy calculation changes during every iteration of the ID3 algorithm. When _H = 0_ (entropy), the set is perfectly classified. Given the nature of our study, the entropy will only range between 0-1 as edible/poisonous is inherently binary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQzflnv6WaDVhi22YfZK2c8nEMY1wIS9TH4fA5Vu1LKcJslYCdj\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQzflnv6WaDVhi22YfZK2c8nEMY1wIS9TH4fA5Vu1LKcJslYCdj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S represents the subset before any splitting, and D represents the possible outcomes of splitting using a given attribute and condition. V assumes that all the values D can be measured one by one. The parallel lines on V and S denote the size of the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Determining the split**\n",
    "Entropy is calculated on all attributes. The minimum entropy will determine where the split will occur on some given set.\n",
    "\n",
    "Alternatively, if information gain is used as a reference for splitting, the node with the highest information gain will determine the split in that iteration for a given set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "## ID3 Algorithm Data Model Design\n",
    "\n",
    "The ID3 algorithm stores the decision tree as a **nested dictionary**, which operates in similar fashion to a standard dictionary (key-value structure) with the difference being that key-value pairs can be nested inside as well.\n",
    "\n",
    "The outer-most dictionary in the nested dictionary structure represents the _root split_ of the tree and therefore only has one key-value pair as it is the intial starting point of the decision tree. The actual nesting begins in the value part of the root's key-value pair.\n",
    "\n",
    "The key-value pair in the ID3 algorithm represents the attribute and it's classification. Once entropy can no longer be reduced (i.e. uncertainty can no longer be decreased for the set), splits will stop occuring and hence nesting of the dictionary will stop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "We import libraries that will implement the ID3 algorithm. These will help us with:\n",
    "+ Data wrangling\n",
    "+ Math modules\n",
    "+ Visualising the actual ID3 algorithm and it's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "import pydot\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data, identifying attributes and class column\n",
    "\n",
    "A function, __import_data__, will allow the ID3 algorithm to separate the target class column from the other attributes in the mushroom data whilst the data is being imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "             data = list(csv.reader(f)) #Format the data as a list\n",
    "                                \n",
    "    feats = data.pop(0)         #Remove first row from data and store in feats\n",
    "    feats.remove(\"class\") #Remove 'class' from feats, so that we are left with only features\n",
    "    target_col = [i.pop(0) for i in data] #Remove first value from each row in data, and store in 'target_col'\n",
    " \n",
    "    check_set_1 = len(data[0]) == len(feats) #Check that the data does not contain the target column\n",
    "    check_set_2 = len(data) == len(target_col) #Check that there is enough observations to match the amount of classifications\n",
    "    \n",
    "    if check_set_1 and check_set_2:\n",
    "        return data, target_col, feats #If both checks are satisfied return each required segment of the data\n",
    "    else:\n",
    "        print(\"Function import_data not working, ERROR\") #If there is an issue with the data, raise the exception\n",
    "        \n",
    "        exit(-1) #If error, exit function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding entropy and information gain formulas\n",
    "\n",
    "##### Entropy\n",
    "\n",
    "Entropy for the target class (edible/poisonous) can be made with two functions: __entropy_formula__ and __entropy_target__.\n",
    "Entropy_formula contains the formula for entropy mentioned earlier. Entropy_target calculates the entropy of the edibility target class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_formula(p): ## Iterates through all percentages in between 0 and 1\n",
    "     if p != 0:\n",
    "        return -p * np.log2(p)\n",
    "     else:\n",
    "        return 0\n",
    "    \n",
    "def entropy_target(target_col): # This function calculates the entropy of our target column i.e. class\n",
    "    unique_class = {} \n",
    "    n = len(target_col) \n",
    "    for i in target_col: \n",
    "        if i not in unique_class: \n",
    "            unique_class[i] = 1\n",
    "        else:\n",
    "            unique_class[i] = unique_class[i] + 1 \n",
    "    entropy = 0\n",
    "    for i in unique_class: # Calculate percentage of target column\n",
    "\n",
    "        p = unique_class[i] / float(n) #entropy function\n",
    "        entropy = entropy + entropy_formula(p) ## Adds all values to zero (0). Sum = target column entropy\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Information Gain\n",
    "Information gain will be implemented using the formula mentioned earlier, as well as using the entropy_formula function to calculate the information gain of each attribute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(data, classes, feat):\n",
    "    gain = 0\n",
    "    n_d = len(data)\n",
    "\n",
    "    fx = {} # Finds the different values for every feature\n",
    "    for i in data:\n",
    "        if i[feat] not in fx.keys():\n",
    "            fx[i[feat]] = 1\n",
    "        else:\n",
    "            fx[i[feat]] = fx[i[feat]] + 1\n",
    "    for f in fx.keys():\n",
    "        feat_entropy = 0\n",
    "        row_index = 0\n",
    "        new_class = {}\n",
    "        count_class = 0\n",
    "\n",
    "        for i in data:\n",
    "            if i[feat] == f:\n",
    "                count_class = count_class + 1\n",
    "                if classes[row_index] in new_class.keys():\n",
    "                    new_class[classes[row_index]] = new_class[classes[row_index]] + 1\n",
    "                else:\n",
    "                    new_class[classes[row_index]] = 1\n",
    "            row_index = row_index + 1\n",
    "\n",
    "        # Calculate information gain using entropy for features\n",
    "        for c in new_class.keys():\n",
    "            feat_entropy = feat_entropy + entropy_formula(float(new_class[c]) / count_class)\n",
    "            \n",
    "        gain = gain + float(fx[f]) / n_d * feat_entropy\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning, creating the sets and the decision tree\n",
    "##### Subset\n",
    "\n",
    "To support the iteration of the ID3 algorithm, subsets need to be created in order to partition the dataset. The subsets get smaller after each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset(data, target_col, feat, f):\n",
    "    new_d = []\n",
    "    new_targ = []\n",
    "    n_feats = len(data[0])\n",
    "    row_index = 0\n",
    "    #This section creates a new subset, based on the feature with the highest information gain ratio.\n",
    "    #The new subset will contain all the data, excluding all the previous splits.\n",
    "    for i in data:\n",
    "        if i[feat] == f:\n",
    "            if feat == 0:\n",
    "                new_row = i[1:]\n",
    "            elif feat == n_feats:\n",
    "                new_row = i[:-1]\n",
    "            else:\n",
    "                new_row = i[:feat]\n",
    "                new_row.extend(i[feat + 1:])\n",
    "\n",
    "            new_d.append(new_row)\n",
    "            new_targ.append(target_col[row_index])\n",
    "        row_index = row_index + 1\n",
    "    return new_targ, new_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree\n",
    "\n",
    "The decision tree can be made with a function __create_tree__ . The previous functions will be used in the development of the decision tree to determine splits, calculate and evaluate the biggest information gains as well as decreases in entropy (uncertainty).\n",
    "\n",
    "Before visualisation of the tree, the tree can be printed into text with another function __print_tree__ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree(data, classes, feats):\n",
    "\n",
    "    n_d = len(data)\n",
    "    n_feats = len(feats)\n",
    "    unique_values = {} =\n",
    "    for c in classes: # finds all values in target class\n",
    "        if c in unique_values.keys():\n",
    "            unique_values[c] = unique_values[c] + 1\n",
    "        else:\n",
    "            unique_values[c] = 1\n",
    "\n",
    "    default = max(unique_values, key=unique_values.get)\n",
    "    if n_d == 0 or n_feats == 0:\n",
    "        return default\n",
    "    elif len(np.unique(classes)) == 1:\n",
    "        return classes[0]\n",
    "    else:\n",
    "        tot_entropy = entropy_target(classes) # Use entropy and information gain to b=find best feature\n",
    "        inform_gain = np.zeros(n_feats)\n",
    "        for feature in range(n_feats):\n",
    "            inf = information_gain(data, classes, feature)\n",
    "            inform_gain[feature] = tot_entropy - inf\n",
    "        best_feat = np.argmax(inform_gain) # index of the best feature\n",
    "        fxs = np.unique(np.transpose(data)[best_feat])\n",
    "        f = feats.pop(best_feat) # Feature Name\n",
    "        tree = {f: {}}\n",
    "        for fx in fxs:\n",
    "            targ, dat = subset(data, classes, best_feat, fx)\n",
    "            sub_tree = create_tree(dat, targ, feats) #ADD SUBTREE TO TREE\n",
    "            tree[f][fx] = sub_tree\n",
    "        return tree\n",
    "    \n",
    "    \n",
    "def print_tree(tree, node):\n",
    "\n",
    "\n",
    "\n",
    "    tree_dict = {}\n",
    "    for root in tree.keys():\n",
    "        split = []\n",
    "        leaves = {}\n",
    "        split_value = []\n",
    "        for k in tree[root]:\n",
    "            if type(tree[root][k]) is not dict:\n",
    "                leaf = tree[root][k]\n",
    "                if leaf in leaves:\n",
    "                    leaves[leaf].append(k)\n",
    "                else:\n",
    "                    leaves[leaf] = [k] =\n",
    "            else: =\n",
    "                split_value.append(k)\n",
    "                print_tree(tree[root][k], k)\n",
    "\n",
    "    if len(split_value) != 0: # PRINTS SPLIT NODE\n",
    "        split.extend(split_value) #\n",
    "    split.append(leaves) # Append value where split occurs\n",
    "    tree_dict[root] = split\n",
    "    print(node, tree_dict, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to return the values of the target class, a function __tree_output__ will be made. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_output(tree, row, feats): # Shows all of class values, used for\n",
    "    if type(tree) is not dict: # authenticate and prediction functions\n",
    "        return str(tree)\n",
    "    else:\n",
    "        for k in tree.keys():\n",
    "            feat_index = feats.index(k)\n",
    "            feat_i = row[feat_index]\n",
    "            return tree_output(tree[k][feat_i], row, feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training & Testing\n",
    "The following functions will be used in order to train the ID3 algorithm in a new function called __training__:\n",
    "+ __import_data__\n",
    "+ __create_tree__\n",
    "\n",
    "Although accuracy is not the best effectiveness metric of a learning model, it will be implemented using another function __authenticate__ that will compare the mushroom training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(csv_file):\n",
    "    data, target_col, feats = import_data(filename=csv_file) # Assign appropriate values from 'import_data()'\n",
    "    tree = create_tree(data, target_col, feats) #\n",
    "    return tree \n",
    "\n",
    "\n",
    "def authenticate(filename, tree):\n",
    "    with open(filename, 'r') as f: #Assign read only file to 'f'\n",
    "        data = list(csv.reader(f)) #Read 'f' as a csv, then convert to a list and assign to 'data'\n",
    "    feats = data.pop(0) #Remove first element from 'data' and assign to 'feats'\n",
    "    feats.remove(\"class\") #Now delete 'class' from list feats\n",
    "    target_col = [i.pop(0) for i in data] #Now remove the first element from each sublist and assign them to 'target_col'\n",
    "\n",
    "    x = 0\n",
    "    n = len(data)\n",
    "    row_num = 0\n",
    "    output_vector = []\n",
    "\n",
    "    for i in data:\n",
    "        out = tree_output(tree, i, feats) #Gets the class value from row i, assigns it to 'out'\n",
    "        output_vector.append(out) #Appends class value to list 'output_vector'\n",
    "        if out == target_col[row_num]: #Check if the class value for row i = class value for target column\n",
    "            x = x + 1\n",
    "        row_num = row_num + 1\n",
    "    return x/float(n) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation\n",
    "\n",
    "After having defined the numerous functions needed, a final function will be created to import the mushroom training and testing datasets calculate relevant metrics and produce a decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final():\n",
    "    mush_train = \"mush_train.csv\" \n",
    "    mush_test = \"mush_test.csv\" \n",
    "    dec_tree = training(mush_train)\n",
    "    accuracy = authenticate(mush_test, dec_tree) \n",
    "    print(\"Accuracy:\", accuracy, \"\\n\")\n",
    "    print(\"DECISION TREE:)\")\n",
    "    print_tree(dec_tree) \n",
    "\n",
    "final()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The execution of the code originally ran smoothly however some bugs encountered and attempts at resolving these bugs were unsuccessful. Hence, a previous capture of the results are shown below which can be replicated with comprehensive bug fixes of the code and data.\n",
    "\n",
    "Accuracy is yielded as 100%. This indicates that there is overfitting in the results and may point to other issues. As mentioned earlier in the report, the results use nested dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/54nrJ36.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://i.imgur.com/54nrJ36.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In conclusion, the ID3 model was originally implemented successfully on the mushroom data however some issues were present. On reflection, the training and testing data should've been completely separate datasets instead of using the training dataset as a subset of the testing data. Ensuring these were distinct would have yielded a more meaningful accuracy to evaluate the effectiveness of the model.\n",
    "\n",
    "The code and bugfixing should've been given more time and effort so that the code could properly execute the results instead of using prior screenshots however external time constraints prevented this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethical\n",
    "Social and ethical applications of the ID3 algorithm can be used in many fields. For example, the mushroom study in this report is a valuable model as it attempts to identify dangerous food items amongst safe ones in a simple scenario. Implementing this type of model on real-life unknown mushrooms in the future may prove beneficial in predicting the outcome of contact before running a hard trial-and-error.\n",
    "\n",
    "In similar fashion, the ID3 algorithm can be implemented on other common discrete valued outcome scenarios and may provide further insights across different industries and fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Pitch\n",
    "[Available in the following Google Drive URL](http://tiny.cc/rali31005a2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
