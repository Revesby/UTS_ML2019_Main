{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML2019_a1_12912616_Ali_Reezvy",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Revesby/UTS_ML2019_Main/blob/master/ML2019_a1_12912616_Ali_Reezvy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upKl6rLQIX44",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# XGBoost: A Scalable Tree Boosting System (2016)\n",
        "\n",
        "Tianqi Chen  \n",
        "Carlos Guestrin \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keN81W0IQJXD",
        "colab_type": "text"
      },
      "source": [
        "Name: Reezvy Ali\n",
        "Student ID: 12912616"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VBZQqO4IhPt",
        "colab_type": "text"
      },
      "source": [
        "# 1. Introduction\n",
        "With the growth of technology and innovation increasing exponentially over time, different wants and needs have also sprouted at an individual, business and industry level. Delivering solutions on common issues such as spam e-mail detection classification, fraud detection (Chen & Guestrin 2016) and more has been achieved through the use of machine learning, and in particular tree boosting. One tree boosting method that is renowned for it's scalability and relatively less investment of resources is XGBoost.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-HX0Ij8Igwx",
        "colab_type": "text"
      },
      "source": [
        "# 2. Content\n",
        "\n",
        "\n",
        "The paper describes XGBoost, a tree boosting method which is a scalable machine learning system. Minimal resource use and scalability of XGBoost is the major proposal for this method of machine learning as it attempts to tackle issues found in the the machine learning field. Problems it attempted to resolve were a novel sparsity aware algorithm for working with sparse data as well as a weighted quantile sketch for approximate learning.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b9xxhhMv96t",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Split Finding Algorithms and Sparse Data\n",
        "\n",
        "In-depth detail is given in the paper on how these problems are proposed to be resolved, such as exact greedy and approximate algorithms which attempt to find the best split in a classic tree learning problem. \n",
        "\n",
        "![](https://wiki.math.uwaterloo.ca/statwiki/images/c/cc/exact_greedy.png)\n",
        "\n",
        "The premise of split finding starts with the basic framework of the *Exact Greedy Algorithm*. The algorithm enumerates across all features and observes all possible splits. However, this algorithm demands resources heavily, especially in the case of continuous features, and is remedied by an *Approximate Algorithm*. Percentiles of feature distribution determine candidate splitting points. The continuous features are mapped into buckets split by the candidate points and the optimal proposal is selected.\n",
        "\n",
        "Sparse data is another issue that XGBoost works around, and *sparsity aware split finding* is possible through this algorithm. Each tree node is given a default direction so that missing data points are classified accordingly. Default directions can be denoted by  optionally learnt default directions which have been learnt from the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GloIeiVwNLh",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 The Block Structure\n",
        "\n",
        "\n",
        "The block structure is a proposed data format to combat the cost of sorting into order. Each block of data is stored in the compressed column format, and each column is sorted by the corresponding feature value. This is especially useful in the case of approximate algorithms, as mentioned above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWG4jITRw_mD",
        "colab_type": "text"
      },
      "source": [
        "# Innovation\n",
        "\n",
        "The paper contributes innovative algorithms and emphasis on comparison between different methods of tree boosting with XGBoost as relatively superior method to other major tree boosting systems. As talked about previous, the sparsity-aware algorithm is designed to handle missing values in the process of split-finding. According to the paper, the '...sparsity aware algorithm runs 50 times faster than the naive version. This confirms the importance of the sparsity aware algorithm' (Chen & Guestrin 2016). \n",
        "\n",
        "As denoted by Table 1 in the paper, the XGBoost tree boosting system is capable of all of the following algorithms/methods: exact greedy, approximate global, approximate local, out-of-core, sparsity aware and parallel in the context of split finding. Other tree boosting systems listed in comparison, such as scikit-learn and R.gbm, support some combination of the listed algorithms but not unanimously.\n",
        "\n",
        "XGBoosts capabilities are highlighted against a dataset with 1.7 billion instances. The entire dataset is handled with only four machines and demonstrates the potential for the system to tackle data on a grander scale. Another outlined example is a comparison of exact greedy methods with 500 trees on the Higgs Boson dataset. XGBoost was compared to scikit-learn and R.gbm with respect to time per tree (sec) and test Area Under the Curve. XGBoost was the fastest in time per tree (0.6841) compared to scikit-learn(28.51) and R.gbm9 (1.032). The test AUC(0.8304) was also greater than both comparisons respectively (0.8302, 0.6224)\n",
        "\n",
        "As denoted above, XGBoost can utilise out-of-core computing. This enables large quantities of instances, such as the 1.7 billion example, to be computed with limited resources which was a benefit of XGBoost emphasised throughout the paper; effectiveness with minimal resource use.\n",
        "\n",
        "Another emphasis of the XGBoost system was scalability. Given this property, XGBoost performance tends to scale linearly as machines are increased in a given problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx_M6845ImHF",
        "colab_type": "text"
      },
      "source": [
        "# Technical Quality\n",
        "\n",
        "The technical quality of the work in the paper is fairly decent. The work in the paper is replicable by someone as the mathematical justification as well as concise explanation of concept and application is denoted fairly well throughout. \n",
        "\n",
        "The comparison between XGBoost and other tree boosting systems/classifiers is done fairly well, as a number of different types of experiments and approaches are undertaken to evaluate performance.  For example, the third performance evaluation is conducted through the renowned **Yahoo! Learning to Rank Challenge** which is a common benchmark in algorithms desinged to learn ranking.\n",
        "\n",
        "\n",
        "One issue that was prominent was performance evaluation metrics. For the most part, individual experiment types had assigned performance metrics, such as time per tree in the comparison of exact greedy methods with 500 trees. However, accuracy is commonly cited which poses an issue as the training set is the basis for the accuracy measurement. This indicates some lacking in quality of performance analysis as accuracy on a training set is expected to be high than the actual test error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEi26BBhItbv",
        "colab_type": "text"
      },
      "source": [
        "# Application and X-factor\n",
        "*Do you think the application domain is appropriate for the proposed technique? What other\n",
        "application domains could the research work be applied? Also in this section, give a couple of suggestions for further\n",
        "developments of the research work. Do you think the work described in the paper could spark a good discussion in\n",
        "class? What did you find interesting about the work?*\n",
        "\n",
        "The application domain is appropriate for XGBoost. The introduction to the paper opens with real-world scale examples of problems that machine learning is useful for, such as fraud detection in banks or spam email detection. The benefits of XGBoost are scalability and minimal resource expenditure. Ultimately, the fate of data and information is that their respective size will scale at an exponential rate and methods of classification/regression need to match this scale in order for useful insights and classifications to continue. \n",
        "\n",
        "Hence, XGBoost is in the appropriate domain for machine learning across all different fields as it is able to utilise sparse data, can use minimal resources and is able to handle large datasets without massive investment, as demonstrated by the dataset with 1.7 billion instances example that was able to be handled by 4 machines.\n",
        "\n",
        "The work described in the paper could spark a good discussion in class as outdated and sub-optimal classifiers are not intriguing discussion points if they do not serve as a foundation for a layman entering the world of machine learning. Interesting discussion and deep learning can be generated from topics centred around machine learning ideas at the forefront of the industry, such as XGBoost.\n",
        "\n",
        "The work is particularly interesting due to how powerful the capabilities of XGBoost are compared to other classifiers, as well as it's overarching relevance to industries for many different types of problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLI1O0fuIxA9",
        "colab_type": "text"
      },
      "source": [
        "# Presentation\n",
        "\n",
        "The article is presented in a structured and logical manner. The argument was somewhat complex to follow as the concepts and mathematics were unfamiliar however further research and reinforcement of concepts made the layout of the report to follow consistently. Content presented in the report was interesting, particularly on split finding algorithms. The presentation style of the actual report was concise and clean, with relevant graphics, section format and general report structure and etiquette being followed. Overall, the XGBoost argument and proposal in this report went very in-depth and conveys a specific yet interesting dive on this aspect of machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDx-qan6Izjo",
        "colab_type": "text"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSRS8bnQP73r",
        "colab_type": "text"
      },
      "source": [
        "#### Paper\n",
        "\n",
        "Chen, T. Guestrin, C. 2016. 'XGBoost: A Scalable Tree Boosting System' [ONLINE] Available at: http://delivery.acm.org/10.1145/2940000/2939785/p785-chen.pdf?ip=14.200.88.132&id=2939785&acc=CHORUS&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1566819063_e3ff8a5c2fc740aac38e297630910fee [Accessed 28 August 2019]\n",
        "\n",
        "#### Image\n",
        "wiki.math 2018. ‘Emojis for everyone.’ [ONLINE] Available at: https://xgboost.readthedocs.io/en/latest/tutorials/model.html [Accessed 28 August 2019]"
      ]
    }
  ]
}